---
title: "Milestone Report"
author: "Nur Seto Dimas"
date: "3/14/2020"
output: html_document
---

## **Overview**  
This project will focus to explore major features in data provided and future plans regarding text prediction application that will be built based on this exploratory analysis.
The [data set] (https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) provided contains multiples languanges text from twitter, blogs and news, however for this project only english text will be used for analysis.

## **Data Preparation**
```{r Preparation, message=TRUE, warning=TRUE}
library(quanteda)
library(stopwords)
library(dplyr)
library(ggplot2)
library(readtext)

# download.file('https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip', 'Coursera-SwiftKey.zip')
# unzip('Coursera-SwiftKey.zip')

#load data set 
twitter <- readtext("en_US.twitter.txt")
news <- readtext("en_US.news.txt")
blogs <- readtext("en_US.blogs.txt")

# creating corpus
corpusTwitter <- corpus(twitter)
corpusNews <- corpus(news)
corpusBlogs <- corpus(blogs)
rm(twitter,news,blogs)

ContentCorpus <- corpusTwitter + corpusNews + corpusBlogs
docnames(ContentCorpus) <- c("Twitter", "Blog", "News")
rm(corpusTwitter, corpusNews, corpusBlogs)

summary(ContentCorpus)
```

In summary result, tokens represent the number of words in the data set. Overall blogs has the most content of words compared to two other sources while twitter has the most sentences.

**Tokenization**
```{r Tokenization, message=TRUE, warning=TRUE}
# constructing tokens
ContentTokens <- tokens(ContentCorpus, remove_numbers = TRUE,
                        remove_punct = TRUE, remove_symbols = TRUE,
                        remove_twitter = TRUE,
                        remove_hyphens = TRUE, remove_url = TRUE)
ContentTokens <- tokens_select(ContentTokens,
                               names(data_int_syllables),
                               selection = 'keep')
ContentTokens <- tokens_select(ContentTokens, stopwords('en'),
                               selection = 'remove', min_nchar = 2)
ContentTokens <- tokens_wordstem(ContentTokens)
ContentTokens <- tokens_tolower(ContentTokens)

# prepares document features matrix
dfm_content <- dfm(ContentTokens)
dfm_content <- dfm_trim(dfm_content, min_docfreq = 2)


```

## **Exploratory Analysis**  
### **Unigram**
```{r Unigram, message=TRUE, warning=TRUE}
# exploratory data analysis

dfm_content %>%
     textstat_frequency(n=10) %>%
     ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_bar(stat = "identity") +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal()
topfeatures(dfm_content)

```

The exploratory result shows the top ten most common words is **just**. This results is after cleansing process which involves removing the common english stopwords. Quanteda conviniently provides the stopwords function to help with this process. 

### **Bigrams**

```{r Bigrams, message=TRUE, warning=TRUE}
twogram <- tokens_ngrams(ContentTokens, n = 2)
dfm_twogram <-dfm(twogram)
rm(twogram)
bigram <- saveRDS(dfm_twogram, file = "bigram.RDS")
dfm_twogram <- readRDS("bigram.RDS")

dfm_twogram %>%
    textstat_frequency(n=10) %>%
    ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
    geom_point() +
    coord_flip() +
    labs(x = NULL, y = "Frequency") +
    theme_minimal()
topfeatures(dfm_twogram)

```



### **Trigrams**
```{r Trigrams, message=TRUE, warning=TRUE}
threegram <- tokens_ngrams(ContentTokens, n = 3)
dfm_threegram <- dfm(threegram)
rm(threegram)
trigram <- saveRDS(dfm_threegram, file = "trigram.RDS")

dfm_trigram <- readRDS("trigram.RDS")

dfm_threegram %>%
    textstat_frequency(n=10) %>%
    ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
    geom_point() +
    coord_flip() +
    labs(x = NULL, y = "Frequency") +
    theme_minimal()



```




## **Coverage Data**
```{r Coverage, message=TRUE, warning=TRUE}
words <- topfeatures(dfm_content, length(dfm_content))
df <- data.frame(words)
df$n <- c(1:nrow(df))
df$total <- cumsum(df$words)
ggplot(df, aes(x =n, y =total)) + geom_line() + xlab('Number of Unique Words') + ylab('Coverage') + ggtitle('Coverage per Number of Unigrams')
```

## **Performance**
```{r object size, message=TRUE, warning=TRUE}
# Objects size
for (thing in ls()) {
        message(thing)
        print(object.size(get(thing)), units='auto')
}
```

